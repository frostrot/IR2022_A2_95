{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/frostrot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing neccesary files\n",
    "\n",
    "import nltk\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove emojis from the text, if any present\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the article connector present in the text\n",
    "\n",
    "def remove_art_connector(text):\n",
    "  article = [\"CAN\",\"IS\",\"HIS\",\"MORE\",\"WHO\",\"ABOUT\",\"THEIR\",\"OUR\",\"HAS\",\"WHO\",\"GET\",\"THEM\",\"WHAT\",\"OUT\",\"FROM\",\"HAVE\",\"HERE\",\"WE\",\"ALL\",\"THERE\",\"TO\",\"ALSO\",\"AND\",\"AS\",\"BUT\",\"YET\",\"YOU\",\"THE\",\"WAS\",\"FOR\",\"ARE\",\"THEY\",\"THIS\",\"THAT\",\"WERE\",\"WITH\",\"YOUR\",\"JUST\",\"WILL\",\"NOT\"]\n",
    "  ans=[]\n",
    "  for word in text:\n",
    "    if word.strip() not in article:\n",
    "      ans.append(word)\n",
    "  return ans\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    x = text.split(\" \")\n",
    "    ps = PorterStemmer()\n",
    "    return \" \".join([ps.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations from the text\n",
    "\n",
    "def remove_punc(tokens):\n",
    "  table = string.punctuation\n",
    "  ptokens = []\n",
    "  for w in tokens:\n",
    "    if w not in table:\n",
    "      ptokens.append(w)\n",
    "  ptokens = [s for s in ptokens if s]\n",
    "  ptokens = [re.sub(r\"[\\n\\t]+\",\" \",s) for s in ptokens]\n",
    "  return ptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords, and convert shorted words into there extended forms\n",
    "\n",
    "def stopword(text):\n",
    "  EXTENDED_FORMS = {\"aren't\": 'are not', \"can't\": 'cannot', \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'll\": 'he will', \"he's\": 'he is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"isn't\": 'is not', \"it's\": 'it is', \"it'll\": 'it will', \"i've\": 'i have', \"let's\": 'let us', \"mightn't\": 'might not', \"mustn't\": 'must not',\"n't\": 'not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"shouldn't\": 'should not', \"that's\": 'that is', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"we'd\": 'we would', \"we're\": 'we are', \"weren't\": 'were not', \"we've\": 'we have', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"where's\": 'where is', \"who'd\": 'who would', \"who'll\": 'who will', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have', \"won't\": 'will not', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', \"'re\": ' are', \"wasn't\": 'was not', \"we'll\": 'we will', \"'cause\": 'because', \"could've\": 'could have', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd've\": 'i would have', \"i'll've\": 'i will have', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll've\": 'it will have', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd've\": 'she would have', \"she'll've\": 'she will have', \"should've\": 'should have', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"this's\": 'this is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"there'd\": 'there would', \"there'd've\": 'there would have', \"here's\": 'here is', \"they'd've\": 'they would have', \"they'll've\": 'they will have', \"to've\": 'to have', \"we'd've\": 'we would have', \"we'll've\": 'we will have', \"what'll've\": 'what will have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where've\": 'where have', \"who'll've\": 'who will have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd've\": 'you would have', \"you'll've\": 'you will have'}\n",
    "  x= word_tokenize(text)\n",
    "  for i in range(len(x)):\n",
    "    if x[i] in EXTENDED_FORMS:\n",
    "      x[i] = EXTENDED_FORMS[x[i]]\n",
    "    if x[i] in stopwords.words('english'):\n",
    "      x[i]=''\n",
    "  x=remove_punc(x)\n",
    "  x=remove_art_connector(x)\n",
    "  return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the parsed text, by, converting them into lowercase, removing any tags, extra spaces.\n",
    "\n",
    "def filter(item):\n",
    "  if type(item)==str:\n",
    "    item=item.lower()\n",
    "    item=re.sub('[#@]\\w+\\s*',\"\",item)\n",
    "    item=re.sub(r'\\\\N','',item)\n",
    "    item=remove_emoji(item)\n",
    "    item=stopword(item)\n",
    "    item=stemming(item)\n",
    "    item = word_tokenize(item)\n",
    "  return set(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected from 1133 files\n",
      "0 files had error\n"
     ]
    }
   ],
   "source": [
    "archive = zipfile.ZipFile('../Humor,Hist,Media,Food.zip', 'r')\n",
    "data = []\n",
    "error_files = []\n",
    "file_list = archive.namelist()[1:]\n",
    "for filename in file_list:\n",
    "    try:\n",
    "        with archive.open(filename,'r') as f:\n",
    "            name = str(filename).split(\"/\")[-1]\n",
    "            textlist = []\n",
    "            for line in io.TextIOWrapper(f,'latin-1'):\n",
    "                textlist.append(line)\n",
    "            content = \" \".join(textlist)\n",
    "            data.append({'file':name,'content':content})\n",
    "    except:\n",
    "        error_files.append(str(filename))\n",
    "\n",
    "print(f\"Data collected from {len(data)} files\")\n",
    "print(f\"{len(error_files)} files had error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [06:02<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for files in tqdm(data):\n",
    "    # print(filter(files['content']))\n",
    "    files['filtered_content'] = filter(files['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pkl.dump(data,open('./pickle_files/data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_files/data.pkl','rb') as f:\n",
    "    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(doc1,doc2):\n",
    "    return len(doc1.intersection(doc2))/len(doc1.union(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japice.bev- 0.01818181818181818\n",
      "strsdiet.txt- 0.016260162601626018\n",
      "jalapast.dip- 0.015625\n",
      "childrenbooks.txt- 0.015503875968992248\n",
      "caramels.des- 0.015384615384615385\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the number of queries\"))\n",
    "\n",
    "for _ in range(n):\n",
    "    query = input(\"Enter the query sentence\")\n",
    "    cleaned_query = filter(query)\n",
    "    result = {}\n",
    "\n",
    "    for doc in data:\n",
    "        jc = jaccard_coeff(doc['filtered_content'],cleaned_query)\n",
    "        if jc in result:\n",
    "            result[jc].append(doc['file'])\n",
    "        else:\n",
    "            result[jc] = [doc['file']] \n",
    "\n",
    "    result = sorted(result.items(),key=lambda x:x[0],reverse=True)\n",
    "    # print(result)\n",
    "    top_k = {}\n",
    "    i=0\n",
    "\n",
    "    while len(top_k)<5:\n",
    "        for value in result[i][1]:\n",
    "            print(value,end=\"- \")\n",
    "            print(result[i][0])\n",
    "            top_k[value]= result[i][0]\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
